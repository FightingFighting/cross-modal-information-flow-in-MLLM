# Cross-modal Information Flow in Multimodal Large Language Model
This is the official repository for the paper: [Cross-modal Information Flow in Multimodal Large Language Models](https://arxiv.org/abs/2411.18620)

# Installation
```
1. git clone https://github.com/FightingFighting/cross-modal-information-flow-in-MLLM.git
2. cd cross-modal-information-flow-in-MLLM
3. Please following LLaVANEXT(https://github.com/LLaVA-VL/LLaVA-NeXT) to install the environment: llava
```
After installing llava environment, you will find an LLaVA-NeXT folder in cross-modal-information-flow-in-MLLM

# Dataset
Our dataset is collected from [GQA](https://cs.stanford.edu/people/dorarad/gqa/index.html). The reformated datasets are in `datasets`.

# Use



## Cite
If this project is helpful for you, please cite our paper:
```
@article{zhang2024cross,
  title={Cross-modal Information Flow in Multimodal Large Language Models},
  author={Zhang, Zhi and Yadav, Srishti and Han, Fengze and Shutova, Ekaterina},
  journal={arXiv preprint arXiv:2411.18620},
  year={2024}
}
```


## Acknowledgement
The code is built upon https://github.com/google-research/google-research/tree/master/dissecting_factual_predictions and [LLaVA](https://github.com/LLaVA-VL/LLaVA-NeXT).
Our used datasets are collect from [GQA](https://cs.stanford.edu/people/dorarad/gqa/index.html)
